时间：2019年6月17日 ~ 2019年6月23日

学习人|学习任务|学习心得和参考资料
------ | ------ | ------ 
333 | 1.学习 python基础  2.学习吴恩达机器学习入门课程  3.学习SQL语句| 已经自己套用别人写过的模型完成了一些工作，但是发现如果还要更进一步的话，还是要更熟练得掌握python，it is very important to get familiar with python。 学习资料用的都是星球大大分享的！
杨建民 | 精读《统计学习方法》的第1章和第2章|学习笔记：https://app.yinxiang.com/fx/acea2b7d-9f11-490f-bcc8-8224f196c4d1 https://app.yinxiang.com/fx/64e2f8ae-d926-4f1b-abb5-3b72307a6d1a
具信 |复习《普林斯顿微积分读本》 1- 9 章  | 巩固数学基础，主要复习以下内容：1.函数的基本概念（定义、区间、奇偶性等） 2.函数的基本操作（利用图像、求极限、连续性、导数等） 3.几种常见函数（重点是三角函数、多项式、指数函数、对数函数以及以e为底的函数）以及它们的法则（极限、等式、不等式、导数等公式）
李小川 | numpy复习 +加权线性回归复习+ 集成方法学习 | 这周的学习比较零散，起初是学习集成方法算法模型，原理是比较简单易理解的，主要分为两种：1、投票选举(bagging: 自举汇聚法 bootstrap aggregating): 是基于数据随机重抽样分类器构造的方法 2、再学习(boosting): 是基于所有分类器的加权求和的方法 。前者的算法模型常用的是随机森林，后者常用算法为AdaBoost。但是在算法实现的过程中，发现numpy 和 pandas的运用还是不够灵活，于是重新看了下numpy的教程。后续的主要打算是先跳过这一节内容，先复习一下线性回归，下周再继续攻下集成方法学习。
陈亮 | 统计学知识 + explore data + ML live | 1.这周主要是学了一些统计学的知识，包括泊松分布，泊松过程还有经验积累分布函数ecdf。2.找了一个感兴趣的dataset关于澳洲2019大选的推特,准备拿这个练习一下，目前还在explore data阶段，发现一些有趣的现象澳洲人喜欢集中在周六发大选相关的twitter。接下来准备分析一下地理位置和推文之间的关系。3.听了一个微软中国cto韦青关于machine learning的live, 对目前ml所处的阶段有了更多的理解。ML大牛Ali Rahimi说目前的AI还处在炼金术时代,这也可以理解为目前ML还没有真正成为科学, 我们对于我们的模型为什么管用很多时候不是很理解，不能给出逻辑上的解释。所以目前进入ML/DL一点都不晚，目前很多的框架比如tensorflow在未来都会成为底层的架构不需要我们进行很多手动的操作，还是要把理论理解的深入一点比较好, 永远都不会过时。框架的熟练使用是一种temporary knowledge, 我们应该把精力更多放在permanent knowledge。就像学CS,算法数据结构才是permanent knowledge。
张弈 | SVM学习 | 之前有大概了解了SVM的基本原理，这周比较系统的学习一下。对于线性可分支持向量机，应用拉格朗日对偶性求解对优化问题；在约束条件上引入松弛变量，求解训练数据集线性不可分的凸二次规划问题，即求解软间隔最大化问题；对于非线性支持向量机，使用核函数将非线性可分问题变成线性可分问题，然后利用线性分类学习方法学习分类模型；使用SMO算法求解凸二次规划的对偶问题。
ziliAI123|本周开始学习吴恩达的机器学习课程，刚开始入门，还需继续努力。python课程的话，我看的是黑马的，现在还在学习Linux。本周开始的比较晚，周五才拿到课程，所以下周开始规定任务，继续努力。
> 注：大家打卡时，为了方便自己和拯救他人，请注意**格式美观**，每段用心编辑的文字，都代表了我们的学习态度。如果表格中无法很好的显示格式，可以在文档后附上打卡内容，如下。
